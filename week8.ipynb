{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 8 Activities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build logistic regression model (**without using scikit-learn**) to predict the rightmost column/feature of `dataset/ionosphere.data.csv` based on remaining features. \n",
    "- Please note: it's a binary classification task. \n",
    "- Try: \n",
    "    - i) batch gradient descent, \n",
    "    - ii) stochastic gradient descent, and \n",
    "    - iii) mini-batch gradient descent algorithms. \n",
    "- Comment on the performances based on learning rate, batch size, etc. \n",
    "- Show binary classification model performance in terms of Accuracy, Precision, Recall, F1-score, ROC (with AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/ionosphere.data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>0.99539</th>\n",
       "      <th>-0.05889</th>\n",
       "      <th>0.85243</th>\n",
       "      <th>0.02306</th>\n",
       "      <th>0.83398</th>\n",
       "      <th>-0.37708</th>\n",
       "      <th>1.1</th>\n",
       "      <th>0.03760</th>\n",
       "      <th>...</th>\n",
       "      <th>-0.51171</th>\n",
       "      <th>0.41078</th>\n",
       "      <th>-0.46168</th>\n",
       "      <th>0.21266</th>\n",
       "      <th>-0.34090</th>\n",
       "      <th>0.42267</th>\n",
       "      <th>-0.54487</th>\n",
       "      <th>0.18641</th>\n",
       "      <th>-0.45300</th>\n",
       "      <th>g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.18829</td>\n",
       "      <td>0.93035</td>\n",
       "      <td>-0.36156</td>\n",
       "      <td>-0.10868</td>\n",
       "      <td>-0.93597</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.04549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26569</td>\n",
       "      <td>-0.20468</td>\n",
       "      <td>-0.18401</td>\n",
       "      <td>-0.19040</td>\n",
       "      <td>-0.11593</td>\n",
       "      <td>-0.16626</td>\n",
       "      <td>-0.06288</td>\n",
       "      <td>-0.13738</td>\n",
       "      <td>-0.02447</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.03365</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00485</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.12062</td>\n",
       "      <td>0.88965</td>\n",
       "      <td>0.01198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.40220</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>-0.22145</td>\n",
       "      <td>0.43100</td>\n",
       "      <td>-0.17365</td>\n",
       "      <td>0.60436</td>\n",
       "      <td>-0.24180</td>\n",
       "      <td>0.56045</td>\n",
       "      <td>-0.38238</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.45161</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.71216</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90695</td>\n",
       "      <td>0.51613</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.20099</td>\n",
       "      <td>0.25682</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.32382</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.02401</td>\n",
       "      <td>0.94140</td>\n",
       "      <td>0.06531</td>\n",
       "      <td>0.92106</td>\n",
       "      <td>-0.23255</td>\n",
       "      <td>0.77152</td>\n",
       "      <td>-0.16399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.65158</td>\n",
       "      <td>0.13290</td>\n",
       "      <td>-0.53206</td>\n",
       "      <td>0.02431</td>\n",
       "      <td>-0.62197</td>\n",
       "      <td>-0.05707</td>\n",
       "      <td>-0.59573</td>\n",
       "      <td>-0.04608</td>\n",
       "      <td>-0.65697</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02337</td>\n",
       "      <td>-0.00592</td>\n",
       "      <td>-0.09924</td>\n",
       "      <td>-0.11949</td>\n",
       "      <td>-0.00763</td>\n",
       "      <td>-0.11824</td>\n",
       "      <td>0.14706</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.01535</td>\n",
       "      <td>-0.03240</td>\n",
       "      <td>0.09223</td>\n",
       "      <td>-0.07859</td>\n",
       "      <td>0.00732</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00039</td>\n",
       "      <td>0.12011</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  0  0.99539  -0.05889  0.85243  0.02306  0.83398  -0.37708      1.1  \\\n",
       "0  1  0  1.00000  -0.18829  0.93035 -0.36156 -0.10868  -0.93597  1.00000   \n",
       "1  1  0  1.00000  -0.03365  1.00000  0.00485  1.00000  -0.12062  0.88965   \n",
       "2  1  0  1.00000  -0.45161  1.00000  1.00000  0.71216  -1.00000  0.00000   \n",
       "3  1  0  1.00000  -0.02401  0.94140  0.06531  0.92106  -0.23255  0.77152   \n",
       "4  1  0  0.02337  -0.00592 -0.09924 -0.11949 -0.00763  -0.11824  0.14706   \n",
       "\n",
       "   0.03760  ...  -0.51171  0.41078  -0.46168  0.21266  -0.34090  0.42267  \\\n",
       "0 -0.04549  ...  -0.26569 -0.20468  -0.18401 -0.19040  -0.11593 -0.16626   \n",
       "1  0.01198  ...  -0.40220  0.58984  -0.22145  0.43100  -0.17365  0.60436   \n",
       "2  0.00000  ...   0.90695  0.51613   1.00000  1.00000  -0.20099  0.25682   \n",
       "3 -0.16399  ...  -0.65158  0.13290  -0.53206  0.02431  -0.62197 -0.05707   \n",
       "4  0.06637  ...  -0.01535 -0.03240   0.09223 -0.07859   0.00732  0.00000   \n",
       "\n",
       "   -0.54487  0.18641  -0.45300  g  \n",
       "0  -0.06288 -0.13738  -0.02447  b  \n",
       "1  -0.24180  0.56045  -0.38238  g  \n",
       "2   1.00000 -0.32382   1.00000  b  \n",
       "3  -0.59573 -0.04608  -0.65697  g  \n",
       "4   0.00000 -0.00039   0.12011  b  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 35)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the last column \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore',drop='first', sparse_output=False)\n",
    "encoded_col = encoder.fit_transform(df[['g']])\n",
    "encoded_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['b', 'g'], dtype=object)]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>0.99539</th>\n",
       "      <th>-0.05889</th>\n",
       "      <th>0.85243</th>\n",
       "      <th>0.02306</th>\n",
       "      <th>0.83398</th>\n",
       "      <th>-0.37708</th>\n",
       "      <th>1.1</th>\n",
       "      <th>0.03760</th>\n",
       "      <th>...</th>\n",
       "      <th>-0.51171</th>\n",
       "      <th>0.41078</th>\n",
       "      <th>-0.46168</th>\n",
       "      <th>0.21266</th>\n",
       "      <th>-0.34090</th>\n",
       "      <th>0.42267</th>\n",
       "      <th>-0.54487</th>\n",
       "      <th>0.18641</th>\n",
       "      <th>-0.45300</th>\n",
       "      <th>g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.18829</td>\n",
       "      <td>0.93035</td>\n",
       "      <td>-0.36156</td>\n",
       "      <td>-0.10868</td>\n",
       "      <td>-0.93597</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.04549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26569</td>\n",
       "      <td>-0.20468</td>\n",
       "      <td>-0.18401</td>\n",
       "      <td>-0.19040</td>\n",
       "      <td>-0.11593</td>\n",
       "      <td>-0.16626</td>\n",
       "      <td>-0.06288</td>\n",
       "      <td>-0.13738</td>\n",
       "      <td>-0.02447</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.03365</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00485</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.12062</td>\n",
       "      <td>0.88965</td>\n",
       "      <td>0.01198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.40220</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>-0.22145</td>\n",
       "      <td>0.43100</td>\n",
       "      <td>-0.17365</td>\n",
       "      <td>0.60436</td>\n",
       "      <td>-0.24180</td>\n",
       "      <td>0.56045</td>\n",
       "      <td>-0.38238</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.45161</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.71216</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90695</td>\n",
       "      <td>0.51613</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.20099</td>\n",
       "      <td>0.25682</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.32382</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.02401</td>\n",
       "      <td>0.94140</td>\n",
       "      <td>0.06531</td>\n",
       "      <td>0.92106</td>\n",
       "      <td>-0.23255</td>\n",
       "      <td>0.77152</td>\n",
       "      <td>-0.16399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.65158</td>\n",
       "      <td>0.13290</td>\n",
       "      <td>-0.53206</td>\n",
       "      <td>0.02431</td>\n",
       "      <td>-0.62197</td>\n",
       "      <td>-0.05707</td>\n",
       "      <td>-0.59573</td>\n",
       "      <td>-0.04608</td>\n",
       "      <td>-0.65697</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02337</td>\n",
       "      <td>-0.00592</td>\n",
       "      <td>-0.09924</td>\n",
       "      <td>-0.11949</td>\n",
       "      <td>-0.00763</td>\n",
       "      <td>-0.11824</td>\n",
       "      <td>0.14706</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.01535</td>\n",
       "      <td>-0.03240</td>\n",
       "      <td>0.09223</td>\n",
       "      <td>-0.07859</td>\n",
       "      <td>0.00732</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00039</td>\n",
       "      <td>0.12011</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  0  0.99539  -0.05889  0.85243  0.02306  0.83398  -0.37708      1.1  \\\n",
       "0  1  0  1.00000  -0.18829  0.93035 -0.36156 -0.10868  -0.93597  1.00000   \n",
       "1  1  0  1.00000  -0.03365  1.00000  0.00485  1.00000  -0.12062  0.88965   \n",
       "2  1  0  1.00000  -0.45161  1.00000  1.00000  0.71216  -1.00000  0.00000   \n",
       "3  1  0  1.00000  -0.02401  0.94140  0.06531  0.92106  -0.23255  0.77152   \n",
       "4  1  0  0.02337  -0.00592 -0.09924 -0.11949 -0.00763  -0.11824  0.14706   \n",
       "\n",
       "   0.03760  ...  -0.51171  0.41078  -0.46168  0.21266  -0.34090  0.42267  \\\n",
       "0 -0.04549  ...  -0.26569 -0.20468  -0.18401 -0.19040  -0.11593 -0.16626   \n",
       "1  0.01198  ...  -0.40220  0.58984  -0.22145  0.43100  -0.17365  0.60436   \n",
       "2  0.00000  ...   0.90695  0.51613   1.00000  1.00000  -0.20099  0.25682   \n",
       "3 -0.16399  ...  -0.65158  0.13290  -0.53206  0.02431  -0.62197 -0.05707   \n",
       "4  0.06637  ...  -0.01535 -0.03240   0.09223 -0.07859   0.00732  0.00000   \n",
       "\n",
       "   -0.54487  0.18641  -0.45300    g  \n",
       "0  -0.06288 -0.13738  -0.02447  0.0  \n",
       "1  -0.24180  0.56045  -0.38238  1.0  \n",
       "2   1.00000 -0.32382   1.00000  0.0  \n",
       "3  -0.59573 -0.04608  -0.65697  1.0  \n",
       "4   0.00000 -0.00039   0.12011  0.0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_list = []\n",
    "ohe_df_features = []\n",
    "ohe_df_feature = pd.DataFrame(encoded_col, columns=list(encoder.categories_[0][1:]))\n",
    "ohe_df_features.append(ohe_df_feature)\n",
    "ohe_dataset = pd.concat(ohe_df_features,axis=1)\n",
    "ohe_dataset.index = df.index\n",
    "#drop the categorical feature columns\n",
    "df = df.drop(columns=['g'] )\n",
    "\n",
    "#merge rest of the dataset with these new onehot features\n",
    "df = df.join(ohe_dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>0.99539</th>\n",
       "      <th>-0.05889</th>\n",
       "      <th>0.85243</th>\n",
       "      <th>0.02306</th>\n",
       "      <th>0.83398</th>\n",
       "      <th>-0.37708</th>\n",
       "      <th>1.1</th>\n",
       "      <th>0.03760</th>\n",
       "      <th>...</th>\n",
       "      <th>0.56811</th>\n",
       "      <th>-0.51171</th>\n",
       "      <th>0.41078</th>\n",
       "      <th>-0.46168</th>\n",
       "      <th>0.21266</th>\n",
       "      <th>-0.34090</th>\n",
       "      <th>0.42267</th>\n",
       "      <th>-0.54487</th>\n",
       "      <th>0.18641</th>\n",
       "      <th>-0.45300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.18829</td>\n",
       "      <td>0.93035</td>\n",
       "      <td>-0.36156</td>\n",
       "      <td>-0.10868</td>\n",
       "      <td>-0.93597</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.04549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.20332</td>\n",
       "      <td>-0.26569</td>\n",
       "      <td>-0.20468</td>\n",
       "      <td>-0.18401</td>\n",
       "      <td>-0.19040</td>\n",
       "      <td>-0.11593</td>\n",
       "      <td>-0.16626</td>\n",
       "      <td>-0.06288</td>\n",
       "      <td>-0.13738</td>\n",
       "      <td>-0.02447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.03365</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00485</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.12062</td>\n",
       "      <td>0.88965</td>\n",
       "      <td>0.01198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.57528</td>\n",
       "      <td>-0.40220</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>-0.22145</td>\n",
       "      <td>0.43100</td>\n",
       "      <td>-0.17365</td>\n",
       "      <td>0.60436</td>\n",
       "      <td>-0.24180</td>\n",
       "      <td>0.56045</td>\n",
       "      <td>-0.38238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.45161</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.71216</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.90695</td>\n",
       "      <td>0.51613</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.20099</td>\n",
       "      <td>0.25682</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.32382</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.02401</td>\n",
       "      <td>0.94140</td>\n",
       "      <td>0.06531</td>\n",
       "      <td>0.92106</td>\n",
       "      <td>-0.23255</td>\n",
       "      <td>0.77152</td>\n",
       "      <td>-0.16399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03286</td>\n",
       "      <td>-0.65158</td>\n",
       "      <td>0.13290</td>\n",
       "      <td>-0.53206</td>\n",
       "      <td>0.02431</td>\n",
       "      <td>-0.62197</td>\n",
       "      <td>-0.05707</td>\n",
       "      <td>-0.59573</td>\n",
       "      <td>-0.04608</td>\n",
       "      <td>-0.65697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02337</td>\n",
       "      <td>-0.00592</td>\n",
       "      <td>-0.09924</td>\n",
       "      <td>-0.11949</td>\n",
       "      <td>-0.00763</td>\n",
       "      <td>-0.11824</td>\n",
       "      <td>0.14706</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03513</td>\n",
       "      <td>-0.01535</td>\n",
       "      <td>-0.03240</td>\n",
       "      <td>0.09223</td>\n",
       "      <td>-0.07859</td>\n",
       "      <td>0.00732</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00039</td>\n",
       "      <td>0.12011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  0  0.99539  -0.05889  0.85243  0.02306  0.83398  -0.37708      1.1  \\\n",
       "0  1  0  1.00000  -0.18829  0.93035 -0.36156 -0.10868  -0.93597  1.00000   \n",
       "1  1  0  1.00000  -0.03365  1.00000  0.00485  1.00000  -0.12062  0.88965   \n",
       "2  1  0  1.00000  -0.45161  1.00000  1.00000  0.71216  -1.00000  0.00000   \n",
       "3  1  0  1.00000  -0.02401  0.94140  0.06531  0.92106  -0.23255  0.77152   \n",
       "4  1  0  0.02337  -0.00592 -0.09924 -0.11949 -0.00763  -0.11824  0.14706   \n",
       "\n",
       "   0.03760  ...  0.56811  -0.51171  0.41078  -0.46168  0.21266  -0.34090  \\\n",
       "0 -0.04549  ... -0.20332  -0.26569 -0.20468  -0.18401 -0.19040  -0.11593   \n",
       "1  0.01198  ...  0.57528  -0.40220  0.58984  -0.22145  0.43100  -0.17365   \n",
       "2  0.00000  ...  1.00000   0.90695  0.51613   1.00000  1.00000  -0.20099   \n",
       "3 -0.16399  ...  0.03286  -0.65158  0.13290  -0.53206  0.02431  -0.62197   \n",
       "4  0.06637  ...  0.03513  -0.01535 -0.03240   0.09223 -0.07859   0.00732   \n",
       "\n",
       "   0.42267  -0.54487  0.18641  -0.45300  \n",
       "0 -0.16626  -0.06288 -0.13738  -0.02447  \n",
       "1  0.60436  -0.24180  0.56045  -0.38238  \n",
       "2  0.25682   1.00000 -0.32382   1.00000  \n",
       "3 -0.05707  -0.59573 -0.04608  -0.65697  \n",
       "4  0.00000   0.00000 -0.00039   0.12011  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns=['g'])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     g\n",
       "0  0.0\n",
       "1  1.0\n",
       "2  0.0\n",
       "3  1.0\n",
       "4  0.0"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df[['g']]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 34) (280, 1)\n",
      "(70, 34) (70, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  0. ...  0.  0.  0.]\n",
      " [ 1.  1.  0. ... -1.  0.  0.]\n",
      " [ 1.  1.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 1.  1.  0. ...  0.  0.  0.]\n",
      " [ 1.  1.  0. ...  0.  0.  0.]\n",
      " [ 1.  1.  0. ...  0.  1.  0.]]\n",
      "(280, 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_2/0wh94zd11cj6smgr_57jgfyc0000gn/T/ipykernel_2793/176166140.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  X_train_np = X_train.to_numpy(dtype=X_train.dtypes[0])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_train_np = X_train.to_numpy(dtype=X_train.dtypes[0])\n",
    "X0 = np.hstack((np.ones([X_train.shape[0],1]), X_train_np))\n",
    "\n",
    "print(X0)\n",
    "print(X0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30858257]\n",
      " [0.71460228]\n",
      " [0.25957391]\n",
      " [0.36399766]\n",
      " [0.97028822]\n",
      " [0.62521903]\n",
      " [0.71381   ]\n",
      " [0.16927362]\n",
      " [0.74902261]\n",
      " [0.50635205]\n",
      " [0.34168416]\n",
      " [0.90253079]\n",
      " [0.5561381 ]\n",
      " [0.37327663]\n",
      " [0.97095971]\n",
      " [0.24894874]\n",
      " [0.2968762 ]\n",
      " [0.52295047]\n",
      " [0.45256836]\n",
      " [0.75916921]\n",
      " [0.18140959]\n",
      " [0.82214448]\n",
      " [0.2206545 ]\n",
      " [0.18255589]\n",
      " [0.10075291]\n",
      " [0.58122317]\n",
      " [0.02557707]\n",
      " [0.93917759]\n",
      " [0.60667319]\n",
      " [0.55862429]\n",
      " [0.41061684]\n",
      " [0.72252992]\n",
      " [0.29045801]\n",
      " [0.98013227]\n",
      " [0.51778838]]\n",
      "[[ 2.91651366e-01]\n",
      " [ 4.57140889e-01]\n",
      " [ 3.77517664e-17]\n",
      " [ 7.73937599e-02]\n",
      " [ 9.70954920e-02]\n",
      " [ 3.47133014e-02]\n",
      " [ 1.49072380e-01]\n",
      " [ 2.68792192e-02]\n",
      " [ 1.06544189e-01]\n",
      " [ 6.52755509e-02]\n",
      " [-7.21939313e-02]\n",
      " [-4.34209474e-02]\n",
      " [-1.44195002e-01]\n",
      " [-7.75424577e-02]\n",
      " [ 8.57608580e-02]\n",
      " [ 1.16664198e-01]\n",
      " [ 8.20737979e-02]\n",
      " [ 1.16713348e-01]\n",
      " [ 8.90243465e-02]\n",
      " [-2.22795239e-01]\n",
      " [ 1.44427386e-01]\n",
      " [-9.39072070e-02]\n",
      " [-1.21491995e-01]\n",
      " [ 1.16081459e-01]\n",
      " [ 2.48679798e-01]\n",
      " [-1.27468116e-02]\n",
      " [-2.10173139e-02]\n",
      " [-3.68665458e-01]\n",
      " [ 1.27246421e-01]\n",
      " [ 1.09248063e-02]\n",
      " [ 6.73909285e-02]\n",
      " [ 9.29074822e-03]\n",
      " [ 5.64623466e-02]\n",
      " [ 1.34065016e-01]\n",
      " [-2.54316917e-01]]\n"
     ]
    }
   ],
   "source": [
    "beta = np.random.random_sample(size = (35,1)) \n",
    "print(beta)\n",
    "beta = np.dot(np.dot(np.linalg.pinv(np.dot(X0.T,X0)), X0.T), y_train)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 1)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our regression model is: y_hat = [0.45714089]x + [0.29165137]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Our regression model is: y_hat = {beta[1]}x + {beta[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. ...  1.  0.  0.]\n",
      " [ 1.  1.  0. ...  1.  1.  0.]\n",
      " [ 1.  1.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 1.  1.  0. ...  0.  0.  0.]\n",
      " [ 1.  0.  0. ...  1. -1.  1.]\n",
      " [ 1.  1.  0. ...  0.  0.  0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_2/0wh94zd11cj6smgr_57jgfyc0000gn/T/ipykernel_2793/423094190.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  X_test_np = X_test.to_numpy(dtype=X_test.dtypes[0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_test_np = X_test.to_numpy(dtype=X_test.dtypes[0])\n",
    "x_test_with_bias = np.hstack((np.ones([X_test.shape[0],1]), X_test_np))\n",
    "print(x_test_with_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 35)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_with_bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: beta = [[0.42091641], [0.23060253], delta = 0.3204462259179937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1001: beta = [[0.33026668], [0.41759689], delta = 0.00017547767172438488\n",
      "Epoch 1307: Batch Gradient Descent Algo Converged\n",
      "Solution: beta = [[0.31531843], [0.4329268], delta = 9.988556654301604e-05\n"
     ]
    }
   ],
   "source": [
    "# initialize beta \n",
    "# [beta_0, beta_1]^T\n",
    "beta = np.random.random_sample(size = (X0.shape[1],1))\n",
    "# beta_old will store beta scores from previous iterations\n",
    "beta_old = beta.copy()\n",
    "\n",
    "# num of sample\n",
    "n = X.shape[0]\n",
    "# learning rate \n",
    "alpha = 0.1\n",
    "reach_convergence = False\n",
    "delta = 0\n",
    "eps = 1e-4\n",
    "\n",
    "# other gradient update loop\n",
    "i = 0\n",
    "while reach_convergence == False:\n",
    "    gradient = (1/n)* np.dot(X0.T , (np.dot(X0,beta) - y_train))\n",
    "    beta = beta - alpha * gradient\n",
    "\n",
    "    #convergence test\n",
    "    delta = np.sqrt(np.sum( (beta_old - beta)**2 ) )\n",
    "    if delta < eps:\n",
    "        reach_convergence = True\n",
    "    if i % 1000 == 0:\n",
    "        print(f'Epoch {i+1}: beta = [{beta[0]}, {beta[1]}, delta = {delta}')\n",
    "    beta_old = beta\n",
    "    i = i + 1\n",
    "print(f'Epoch {i+1}: Batch Gradient Descent Algo Converged')\n",
    "print(f'Solution: beta = [{beta[0]}, {beta[1]}, delta = {delta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 1)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now predict with it\n",
    "y_pred_batch = np.dot(x_test_with_bias,beta)\n",
    "y_pred_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = g    0.391413\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalienguyen/Developer/Machine Learning/venv-machine-learning/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:84: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(np.sum((y_test-y_pred_batch)**2)/len(y_test))\n",
    "print(f'RMSE = {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2-score = g    0.343793\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "r2score = 1 - (np.sum((y_test-y_pred_batch)**2)/ np.sum((y_test-np.mean(y_test))**2))\n",
    "print(f'R2-score = {r2score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 1)\n"
     ]
    }
   ],
   "source": [
    "# initialize beta\n",
    "# [beta_0, beta_1]^T\n",
    "beta = np.random.random_sample(size = (X0.shape[1],1)) \n",
    "print(beta.shape)\n",
    "# this will store beta scores from previous iteration\n",
    "beta_old = beta.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num of sample\n",
    "n = X0.shape[0]\n",
    "#learning rate\n",
    "alpha = 0.01\n",
    "reach_convergence = False\n",
    "delta = 0 #distance between two betas\n",
    "eps = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle training set, or keep a record of shuffled indices\n",
    "indices = np.arange(n)\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: beta = [[0.52264863], [0.13729973], delta = 0.004907200382411154\n",
      "Step 1001: beta = [[0.43559637], [0.27382971], delta = 0.00901542619688162\n",
      "Step 2001: beta = [[0.43218927], [0.32549141], delta = 0.004528899262246173\n",
      "Step 3001: beta = [[0.39223896], [0.33405633], delta = 0.04553446274335373\n",
      "Step 4001: beta = [[0.37458887], [0.35861576], delta = 0.0038500573585545476\n",
      "Step 5001: beta = [[0.34719028], [0.36767716], delta = 0.022451031323217956\n",
      "Step 6001: beta = [[0.3426929], [0.3869185], delta = 0.0039019104435788258\n",
      "Step 7001: beta = [[0.34826336], [0.41408427], delta = 0.0034295021027179635\n",
      "Step 8001: beta = [[0.30900374], [0.38241811], delta = 0.004453014206616602\n",
      "Step 9001: beta = [[0.32346345], [0.42489175], delta = 0.0038047757213068517\n",
      "Step 10001: beta = [[0.31557556], [0.43295613], delta = 0.009503600501771826\n",
      "Step 11001: beta = [[0.30890829], [0.43832717], delta = 0.003647582158228438\n",
      "Step 12001: beta = [[0.32791558], [0.45616063], delta = 0.0077597709003181786\n",
      "Step 13001: beta = [[0.29967412], [0.44639858], delta = 0.003456468135294326\n",
      "Step 14001: beta = [[0.31078406], [0.44999469], delta = 0.034044020990644706\n",
      "Step 15001: beta = [[0.30018424], [0.44750218], delta = 0.003788232062369065\n",
      "Step 16001: beta = [[0.31147094], [0.44253536], delta = 0.0031972697700671142\n",
      "Step 17001: beta = [[0.29701081], [0.44440756], delta = 0.0037315270204608046\n",
      "Step 18001: beta = [[0.30583682], [0.4479998], delta = 0.0036551906670631096\n",
      "Step 19001: beta = [[0.31287624], [0.45689407], delta = 0.0033223872163963626\n",
      "Step 20001: beta = [[0.29791788], [0.44615853], delta = 0.0036931694821736145\n",
      "Step 21001: beta = [[0.29869486], [0.44702335], delta = 0.015153144582379706\n",
      "Step 22001: beta = [[0.27869046], [0.43679541], delta = 0.004105752374822515\n",
      "Step 23001: beta = [[0.30644207], [0.4527757], delta = 0.003474668277250886\n",
      "Step 24001: beta = [[0.30755374], [0.4540252], delta = 0.03261509817687825\n",
      "Step 25001: beta = [[0.2780521], [0.44526607], delta = 0.03135476711493924\n",
      "Step 26001: beta = [[0.28522442], [0.45463566], delta = 0.017970736745529754\n",
      "Step 27001: beta = [[0.33430997], [0.48195879], delta = 0.002747402519470623\n",
      "Step 28001: beta = [[0.32399888], [0.48202791], delta = 0.008885202136974812\n",
      "Step 29001: beta = [[0.30454671], [0.46996012], delta = 0.0032540356520808452\n",
      "Step 30001: beta = [[0.3164722], [0.4783312], delta = 0.011469609596941436\n",
      "Step 31001: beta = [[0.27367505], [0.43880974], delta = 0.011904897884322788\n",
      "Step 32001: beta = [[0.29288285], [0.46413479], delta = 0.013207625778589374\n",
      "Step 33001: beta = [[0.30148304], [0.46660302], delta = 0.0033466922827366566\n",
      "Step 34001: beta = [[0.28865995], [0.44991074], delta = 0.003772621260211525\n",
      "Step 35001: beta = [[0.28710394], [0.44332529], delta = 0.003890108482226832\n",
      "Step 36001: beta = [[0.28653782], [0.45477198], delta = 0.016564966870477302\n",
      "Step 37001: beta = [[0.2721011], [0.4212201], delta = 0.010005145330081656\n",
      "Step 37629, Epoch 134: Stochastic Gradient Descent Algo Converged\n",
      "Solution: beta = [[0.28241121], [0.4407967], delta = 6.282370508765517e-07\n"
     ]
    }
   ],
   "source": [
    "#The gradient update loop\n",
    "i = 0\n",
    "iter = 0\n",
    "num_epochs = 0\n",
    "while reach_convergence == False:\n",
    "    #grab a sample\n",
    "    x_sample = X0[indices[i],:].reshape(1,-1) #a row vector, not a column vector\n",
    "    #compute gradient\n",
    "    gradient = np.dot(x_sample.T, (np.dot(x_sample, beta) - y_train.iloc[int(indices[i])].values[0]))\n",
    "    beta = beta - alpha * gradient\n",
    "\n",
    "    #convergence test\n",
    "    delta = np.sqrt(np.sum( (beta_old - beta)**2 ) )\n",
    "    if delta < eps:\n",
    "        reach_convergence = True\n",
    "    if iter % 1000 == 0:\n",
    "        print(f'Step {iter+1}: beta = [{beta[0]}, {beta[1]}, delta = {delta}')\n",
    "    beta_old = beta\n",
    "    i = i + 1\n",
    "    iter = iter + 1\n",
    "    if i >= n: # next epoch begins\n",
    "        np.random.shuffle(indices)\n",
    "        num_epochs = num_epochs + 1\n",
    "        i = 0 #reset\n",
    "print(f'Step {iter+1}, Epoch {num_epochs}: Stochastic Gradient Descent Algo Converged')\n",
    "print(f'Solution: beta = [{beta[0]}, {beta[1]}, delta = {delta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 1)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_stochastic = np.dot(x_test_with_bias,beta)\n",
    "y_pred_stochastic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = g    0.400306\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(np.sum((y_test-y_pred_stochastic)**2)/len(y_test))\n",
    "print(f'RMSE = {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2-score = g    0.313636\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "r2score = 1 - (np.sum((y_test-y_pred_stochastic)**2)/ np.sum((y_test-np.mean(y_test))**2))\n",
    "print(f'R2-score = {r2score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 1)\n"
     ]
    }
   ],
   "source": [
    "#initialize beta\n",
    "beta = np.random.random_sample(size = (X0.shape[1],1)) #[beta_0, beta_1]^T\n",
    "print(beta.shape)\n",
    "beta_old = beta.copy() #this will store beta scores from previous iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num of sample\n",
    "n = X.shape[0]\n",
    "#learning rate\n",
    "alpha = 0.01\n",
    "reach_convergence = False\n",
    "delta = 0 #distance between two betas\n",
    "eps = 1e-3\n",
    "b = 5 #batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle training set, or keep a record of shuffled indices\n",
    "indices = np.arange(n)\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the batch of indices\n",
    "batch_of_indices = []\n",
    "if n%b == 0:\n",
    "    num_of_batches = int(n / b)\n",
    "else:\n",
    "    num_of_batches = n // b + 1\n",
    "idx = 0\n",
    "for _ in range(num_of_batches):\n",
    "    batch_of_indices.append(indices[idx:idx+b])\n",
    "    idx = idx + b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Developer/Machine Learning/venv-machine-learning/lib/python3.9/site-packages/pandas/core/indexing.py:1714\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1716\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Machine Learning/venv-machine-learning/lib/python3.9/site-packages/pandas/core/generic.py:4153\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   4144\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4145\u001b[0m \u001b[38;5;124;03mInternal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[1;32m   4146\u001b[0m \u001b[38;5;124;03mattribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4151\u001b[0m \u001b[38;5;124;03mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   4152\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 4153\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4154\u001b[0m \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Machine Learning/venv-machine-learning/lib/python3.9/site-packages/pandas/core/generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[1;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[1;32m   4131\u001b[0m     )\n\u001b[0;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4135\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4140\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/Machine Learning/venv-machine-learning/lib/python3.9/site-packages/pandas/core/internals/managers.py:891\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    890\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[axis]\n\u001b[0;32m--> 891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_convert_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n",
      "File \u001b[0;32m~/Developer/Machine Learning/venv-machine-learning/lib/python3.9/site-packages/pandas/core/indexers/utils.py:282\u001b[0m, in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n, verify)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n",
      "\u001b[0;31mIndexError\u001b[0m: indices are out-of-bounds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[228], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m reach_convergence \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#grab a batch of samples\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     X_sample \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc[batch_of_indices[i], :]\n\u001b[0;32m----> 8\u001b[0m     y_sample \u001b[38;5;241m=\u001b[39m \u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_of_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#print(f'X_sample.shape = {X_sample.shape}, y_sample.shape = {y_sample.shape}')\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#compute gradient\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mb)\u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X_sample\u001b[38;5;241m.\u001b[39mT , (np\u001b[38;5;241m.\u001b[39mdot(X_sample,beta) \u001b[38;5;241m-\u001b[39m y_sample))\n",
      "File \u001b[0;32m~/Developer/Machine Learning/venv-machine-learning/lib/python3.9/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Machine Learning/venv-machine-learning/lib/python3.9/site-packages/pandas/core/indexing.py:1743\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;66;03m# a list of integers\u001b[39;00m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_list_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# a single integer\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1747\u001b[0m     key \u001b[38;5;241m=\u001b[39m item_from_zerodim(key)\n",
      "File \u001b[0;32m~/Developer/Machine Learning/venv-machine-learning/lib/python3.9/site-packages/pandas/core/indexing.py:1717\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1716\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[39;00m\n\u001b[0;32m-> 1717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional indexers are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "iter = 0\n",
    "num_epochs = 0\n",
    "while reach_convergence == False:\n",
    "    #grab a batch of samples\n",
    "    X_sample = X.iloc[batch_of_indices[i], :]\n",
    "\n",
    "    y_sample = y_train.iloc[batch_of_indices[i]].values.reshape(-1, 1)\n",
    "\n",
    "    #print(f'X_sample.shape = {X_sample.shape}, y_sample.shape = {y_sample.shape}')\n",
    "    \n",
    "    #compute gradient\n",
    "    gradient = (1/b)* np.dot(X_sample.T , (np.dot(X_sample,beta) - y_sample))\n",
    "    beta = beta - alpha * gradient\n",
    "\n",
    "    #convergence test\n",
    "    delta = np.sqrt(np.sum( (beta_old - beta)**2 ) )\n",
    "    if delta < eps:\n",
    "        reach_convergence = True\n",
    "    if iter % 1000 == 0:\n",
    "        print(f'Step {iter+1}: beta = [{beta[0]}, {beta[1]}, delta = {delta}')\n",
    "    beta_old = beta\n",
    "    i = i + 1\n",
    "    iter = iter + 1\n",
    "    if i >= num_of_batches: # next epoch begins\n",
    "        np.random.shuffle(indices)\n",
    "        #create the batch of indices (again)\n",
    "        batch_of_indices = []\n",
    "        if n%b == 0:\n",
    "            num_of_batches = int(n / b)\n",
    "        else:\n",
    "            num_of_batches = n // b + 1\n",
    "        idx = 0\n",
    "        for _ in range(num_of_batches):\n",
    "            batch_of_indices.append(indices[idx:idx+b])\n",
    "            idx = idx + b\n",
    "        #increment epoch counter\n",
    "        num_epochs = num_epochs + 1\n",
    "        i = 0 #reset\n",
    "print(f'Step {iter+1}, Epoch {num_epochs}: Mini-batch Gradient Descent Algo (with batch_size = {b}) Converged')\n",
    "print(f'Solution: beta = [{beta[0]}, {beta[1]}, delta = {delta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
